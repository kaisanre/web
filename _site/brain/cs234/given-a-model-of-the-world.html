<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="lecture 2 - given a model of the world">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="Resources: Lecture Video">
<meta property="og:description" content="Resources: Lecture Video">
<link rel="canonical" href="http://localhost:4000/brain/cs234/given-a-model-of-the-world">
<meta property="og:url" content="http://localhost:4000/brain/cs234/given-a-model-of-the-world">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-05-02T00:00:00-04:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="lecture 2 - given a model of the world">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2024-05-02T00:00:00-04:00","datePublished":"2024-05-02T00:00:00-04:00","description":"Resources: Lecture Video","headline":"lecture 2 - given a model of the world","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/brain/cs234/given-a-model-of-the-world"},"url":"http://localhost:4000/brain/cs234/given-a-model-of-the-world"}</script><title> lecture 2 - given a model of the world - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.png">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="http://localhost:4000/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">writing</a></li>
<li><a href="/about">about</a></li>
<li><a href="/search">search</a></li>
<li><a href="/brain">brain</a></li>
</ul></nav></header><section class="post"><h2>
<a href="/brain/cs234" class="url">cs234</a> / lecture 2 - given a model of the world</h2>
<p><strong>Resources:</strong></p>
<ul><li><a href="https://youtu.be/E3f2Camj0Is?feature=shared">Lecture Video</a></li></ul>
<h3 id="markov-process-and-markov-chains">Markov Process and Markov Chains</h3>
<ul>
<li>Memoryless / random sequence of events which satisfies the markov property</li>
<li>No rewards, no actions</li>
<li>Dynamics model specifies the probability of the next state given the previous state. This is expressible as a matrix: $$ \begin{pmatrix} P(s_1 \vert s_1) &amp; P(s_1 \vert s_2) &amp; \dots &amp; P(s_1 \vert s_N)<br> \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ P(s_N \vert s_1) &amp; P(s_N \vert s_2) &amp; \dots &amp; P(s_N \vert s_N) \end{pmatrix} $$</li>
</ul>
<h3 id="markov-reward-process">Markov Reward Process</h3>
<ul>
<li>Markov Reward Process: Markov Chains + rewards (no actions)</li>
<li>$S:$ Finite number of states ($s \in S$)</li>
<li>$P:$ Dynamics / transition model where $P(s _{t+1} = s’ \vert s_t = s)$</li>
<li>$R:$ Reward function $R(s_t = s) = E[r_t \vert s_t = s]$<ul><li>Tied to immediate state or state and next state</li></ul>
</li>
<li>$\gamma:$ Discount factor [0, 1]</li>
</ul>
<h3 id="return-and-value-function">Return and Value Function</h3>
<ul>
<li>
<strong>Horizon:</strong> The number of time steps in an episodes<ul><li>Can be infinite or finite (finite Markov Process)</li></ul>
</li>
<li>
<strong>Return:</strong> Discounted sum of rewards from time step t to horizon<ul><li>$G_t = r_t + \gamma r _{t+1} + \gamma^2 r _{t+2} \dots$</li></ul>
</li>
<li>
<strong>Value Function:</strong> Expected return from a state<ul>
<li>$V(s) = E[G_t \vert s_t = s] = E[r_t + \gamma r _{t+1} + \gamma^2 r _{t+2} \dots \vert s_t = s]$</li>
<li>Same as return if the process is deterministic (single next state from a state)</li>
<li>Different from return if process is stochastic</li>
</ul>
</li>
</ul>
<h3 id="discount-factor">Discount Factor</h3>
<ul>
<li>Used to avoid inifinite returns and values</li>
<li>$\gamma = 0:$ Only care about immediate rewards</li>
<li>$\gamma = 1:$ Future reward and immediate reward equally beneficial<ul><li>Use $\gamma = 1$ for finite episode lengths (for mathematical convenience)</li></ul>
</li>
</ul>
<h3 id="computing-value-of-a-markov-reward-process">Computing Value of a Markov Reward Process</h3>
<ul>
<li>Estimate by simulation by generating a large number of episodes<ul>
<li>Average the returns</li>
<li>Requires no assumption of Markov structure</li>
</ul>
</li>
<li>Using markov structure<ul><li>MRP Value Function: $V(s) = R(s) + \gamma \sum _{s’ \in S}P(s’ \vert s)V(s’)$<ul>
<li>$R(s):$ Immediate rewards</li>
<li>$\gamma \sum _{s’ \in S}P(s’ \vert s)V(s’):$ Discounted sum of future rewards</li>
</ul>
</li></ul>
</li>
<li>Matrix form of Bellman Equation for MRPs: $V = R + \gamma PV$<ul><li>Simplify equation to: $V - \gamma PV = R = (I - \gamma P)V = R \rightarrow V = (I - \gamma P)^{-1}R$</li></ul>
</li>
</ul>
<p>$$ \begin{pmatrix} V(s_1)<br> \\ \vdots \\ V(s_N) \end{pmatrix} = \begin{pmatrix} R(s_1)<br> \\ \vdots \\ R(s_N) \end{pmatrix} + \gamma \begin{pmatrix} P(s_1 \vert s_1) &amp; P(s_1 \vert s_2) &amp; \dots &amp; P(s_1 \vert s_N)<br> \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ P(s_N \vert s_1) &amp; P(s_N \vert s_2) &amp; \dots &amp; P(s_N \vert s_N) \end{pmatrix} \begin{pmatrix} V(s_1)<br> \\ \vdots \\ V(s_N) \end{pmatrix} $$</p>
<ul><li>Iterative Algorithm for Computing Value of an MRP (Dynamic Programming)<ul>
<li>Initialize $V_0(s) = 0$ for all s</li>
<li>For k = 1 until convergence<ul><li>For all s in S<ul><li>$V_k = R(s) + \gamma \sum _{s’ \in S}P(s’\vert s)V _{k-1}(s’)$</li></ul>
</li></ul>
</li>
</ul>
</li></ul>
<h3 id="markov-decision-processes">Markov Decision Processes</h3>
<ul>
<li>Markov reward processes with actions ($a \in A$)</li>
<li>We have a dynamics model that is specified differently for <em>each action</em> $\rightarrow P(s _{t+1} = s’ \vert s_t = s, a_t = a)$<ul><li>Multiple matrices (one for each action)</li></ul>
</li>
<li>Our reward function can be a function current state, current state + action, or current state + action + next state<ul><li>$R(s_t = s, a_t = a) = E[r_t \vert s_t = s, a_t = a]$</li></ul>
</li>
<li>Can express MDPs as a tuple of $(S, A, R, P, \gamma)$</li>
</ul>
<p><strong>Policies</strong></p>
<ul>
<li>Expressed as $\pi(a_t = a \vert s_t = s) = P(a_t = a \vert s_t = s)$</li>
<li>Specifies which action to take in each state $\rightarrow$ can be deterministic or stochastic</li>
<li>Markov Reward Process = MDP + Policy $\rightarrow$ specifying a policy induces MRP because it defines the expected rewards and transition model<ul>
<li>$R^{\pi}(s) = \sum _{a \in A} \pi(a \vert s)R(s,a)$</li>
<li>$P^{\pi}(s) = \sum _{a \in A} \pi(a \vert s)P(s’\vert s,a)$</li>
</ul>
</li>
</ul>
<h3 id="policy-evaluation">Policy Evaluation</h3>
<ul><li>Iterative Algorithm (Bellman Backup)<ul>
<li>Initialize $V_0(s) = 0$ for all s</li>
<li>For k = 1 until convergence<ul><li>For all s in S<ul><li>$V^{\pi}_k = r(s, \pi(s)) + \gamma \sum _{s’ \in S}P(s’\vert s, \pi(s))V^{\pi} _{k-1}(s’)$</li></ul>
</li></ul>
</li>
</ul>
</li></ul>
<h3 id="mdp-control">MDP Control</h3>
<ul>
<li>We want to find the optimal policy<ul>
<li>$\pi^*(s) = argmax _{\pi} V^{\pi}(s)$</li>
<li>There exists a unique optimal value function</li>
</ul>
</li>
<li>Optimal Policy Characteristics (for infinite horizon MDPs):<ul>
<li>Deterministic</li>
<li>Stationary (doesn’t depend on time step)</li>
<li>not necessarily unique (there could be ties between policies that get the optimal value function)</li>
</ul>
</li>
</ul>
<h3 id="policy-search">Policy Search</h3>
<ul>
<li>We know that the number of deterministic policies is $\vert A \vert ^{\vert S \vert}$</li>
<li>Using enumeration is inefficient (evaluating every policy exhaustively)</li>
<li>We prefer policy iteration</li>
</ul>
<h3 id="mdp-policy-iteration"><a href="#mdp-policy-iteration">MDP Policy Iteration</a></h3>
<ul>
<li>We take a “guess” of the optimal policy, we evaluate it, and then we try to improve it until we cannot improve it anymore</li>
<li>Algorithm:<ul>
<li>Set i = 0</li>
<li>Intialize $\pi_{0}(s)$ randomly for all states $s$</li>
<li>While i == 0 or $\vert\vert \pi_i - \pi _{i-1}\vert\vert &gt; 0$ (L1 Norm to see if policy changes)<ul>
<li>$V^{\pi i} \leftarrow \text{MDP Value Function}$</li>
<li>$\pi _{i+1} \leftarrow \text{Policy Improvement}$</li>
<li>$i = i + 1$</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>State Value:</strong> $V^{\pi}(s)$<ul><li><em>“If you start in state s and follow a policy, what is the discounted sum of rewards”</em></li></ul>
</li>
<li>
<strong>State-Action Value:</strong> $Q^{\pi}(s, a) = R(s, a) + \gamma \sum _{s’ \in S} P(s’ \vert s, a)V^{\pi}(s’)$<ul><li><em>“If you start in state s, take an action, and then follow a policy, what is the discounted sum of rewards”</em></li></ul>
</li>
<li>
<strong>Policy Improvement:</strong><ul>
<li>Compute the state action value of a policy<ul><li>For s in S and a in A<ul><li>Compute $Q ^{\pi i}(s,a) = R(s, a) + \gamma \sum _{s’ \in S} P(s’ \vert s, a)V^{\pi}(s’)$</li></ul>
</li></ul>
</li>
<li>Compute new policy $\pi _{i+1}$ for all $s \in S$<ul><li>$\pi _{i+1} = argmax _{a} Q^{\pi i}(s, a) \forall s \in S$</li></ul>
</li>
</ul>
</li>
<li>Monotonic Improvement in Policy: The value of the policy is greater than or equal to the old policy for all states</li>
<li>Once the policy stops changing, you know you are at the global best policy</li>
<li>There is a maximum of $\vert A \vert ^{\vert S \vert}$ iterations for policy iteration</li>
</ul>
<h3 id="value-iteration">Value Iteration</h3>
<ul>
<li>Maintain the optimal value of starting in a state if we have a finite number of steps left in the episode</li>
<li>Value function must satisfy the bellman equation</li>
<li>
<strong>Bellman Backup Operator:</strong> Allows us to transform an old value function into a new one to improve it<ul>
<li>$BV(s) = max_a R(s, a) + \gamma \sum _{s’ \in S}P(s’ \vert s, a)V(s’)$</li>
<li>For a particular policy: $B^{\pi}V(s) = R^{\pi}(s) + \gamma \sum _{s’ \in S}P^{\pi}(s’ \vert s)V(s)$<ul><li>We can do policy evaluation using this operating as follows: $B^{\pi}B^{\pi}B^{\pi}\cdots V$ until it converges</li></ul>
</li>
</ul>
</li>
<li>Value Iteration Algorithm<ul>
<li>Set k = 1</li>
<li>Initialize $V_0(s) = 0$ for all s</li>
<li>Loop until finite horizon ends or convergence<ul><li>For each state, s:<ul>
<li>Compute $V _{k+1} = max_a R(s, a) + \gamma \sum _{s’ \in S} P(s’ \vert s, a)V_k(s’)$</li>
<li>$V _{k+1} = BV_k$</li>
<li>$\pi _{k+1}(s) = argmax_a R(s,a) + \gamma \sum _{s’ \in S} P(s’ \vert s, a)V_k(s’)$</li>
</ul>
</li></ul>
</li>
</ul>
</li>
<li>Contraction Operator<ul>
<li>Let $O$ be an operator $\vert x \vert$ be any norm of $x$</li>
<li>If $\vert OV - OV’ \vert \leq \vert V - V’\vert$ then O is a contraction operator</li>
<li>Value iteration converges because the Bellman Backup is a contractor operator when $\gamma &lt; 1$</li>
</ul>
</li>
</ul></section></main></body>
</html>
