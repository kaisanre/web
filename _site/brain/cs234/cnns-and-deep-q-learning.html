<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="lecture 6 - cnns and deep q learning">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="Resources: Lecture Video">
<meta property="og:description" content="Resources: Lecture Video">
<link rel="canonical" href="http://localhost:4000/brain/cs234/cnns-and-deep-q-learning">
<meta property="og:url" content="http://localhost:4000/brain/cs234/cnns-and-deep-q-learning">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-05-09T00:00:00-04:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="lecture 6 - cnns and deep q learning">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2024-05-09T00:00:00-04:00","datePublished":"2024-05-09T00:00:00-04:00","description":"Resources: Lecture Video","headline":"lecture 6 - cnns and deep q learning","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/brain/cs234/cnns-and-deep-q-learning"},"url":"http://localhost:4000/brain/cs234/cnns-and-deep-q-learning"}</script><title> lecture 6 - cnns and deep q learning - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.png">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="http://localhost:4000/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">writing</a></li>
<li><a href="/about">about</a></li>
<li><a href="/search">search</a></li>
<li><a href="/brain">brain</a></li>
</ul></nav></header><section class="post"><h2>
<a href="/brain/cs234" class="url">cs234</a> / lecture 6 - cnns and deep q learning</h2>
<p><strong>Resources:</strong></p>
<ul><li><a href="https://youtu.be/gOV8-bC1_KU?feature=shared">Lecture Video</a></li></ul>
<h3 id="limitations-to-linear-value-function-approximation">Limitations to Linear Value Function Approximation</h3>
<ul>
<li>Assumes that the value function is a weighted combination of a set of features where each feature is a function of the state</li>
<li>Requires carefully hand designing a feature set<ul><li>Would be much better if you could go from states without needing a specific feature set</li></ul>
</li>
<li>Local representations (like kernel approaches) doesn’t scale well to enormous state spaces and datasets</li>
</ul>
<h3 id="deep-neural-networks">Deep Neural Networks</h3>
<ul><li>Composition of multiple functions</li></ul>
<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoiZ3JhcGggVEQ7XG54IC0tPiBoMTtcbncxIC0tPiBoMTtcbmgxIC0tPiBoMjtcbncyIC0tPiBoMjtcbmgyIC0tPiAuLi47XG4uLi4gLS0-IGhuO1xud24gLS0-IGhuO1xuaC4uLm4gLS0-IHk7XG55IC0tPiBKO1xuJSUtIiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifX0"></p>
<ul>
<li>$J$ is the loss function</li>
<li>$y = h_n(h _{n-1} (\dots h_1(\overrightarrow{x})))$</li>
<li>Backpropgates gradient using the chain rule<ul><li>The h functions must be differentiable<ul>
<li>Linear: $h_n = w*h _{n-1}$</li>
<li>Nonlinear: $h_n = f(h _{n-1})$ (activation functions like sigmoid or ReLU) <strong>Benefits:</strong>
</li>
</ul>
</li></ul>
</li>
<li>Uses distributed representations instead of local ones</li>
<li>Universal function approximator</li>
<li>Potentially need exponential fewer nodes/parameters (compared to a shallow net) to represent the same function</li>
<li>Can learn parameters using stochastic gradient descent</li>
</ul>
<h3 id="convolutional-neural-networks">Convolutional Neural Networks</h3>
<ul>
<li>Fully Connected Network: Requires an enormous amount of data for visual data<ul>
<li>High space-time complexity</li>
<li>Lack of structure + locality of info</li>
</ul>
</li>
<li>Convolutional Neural Networks<ul>
<li>Considers the local structure + extraction of features</li>
<li>Not fully connected</li>
<li>Locality of processing</li>
<li>Weight sharing for parameter reduction<ul><li>Local parameters that are idential for groups of pixels in the image</li></ul>
</li>
<li>Learns the parameters of multiple convolutional filter banks</li>
<li>Compress to extract salient features and favors generalization</li>
</ul>
</li>
<li>Locality of Information<ul>
<li>Receptive Field: An input patch of where the hidden unit is connected to</li>
<li>Stride: How much you move the patch</li>
<li>Zero Padding: How many 0s to add to either side of an input layer</li>
<li>Activation value of the hidden layer neuron: $g(b + \sum _i w_ix_i)$</li>
</ul>
</li>
<li>Feature Maps:<ul>
<li>All neurons in the first hidden layer capture the same feature just at different locations in hte feature map</li>
<li>Feature: A pattern that makes a neuron produce a certain response level</li>
</ul>
</li>
<li>Pooling Layers:<ul>
<li>Used immediately after covolutional layers</li>
<li>Simplifies / compresses infomration in the output from a convolutional layer</li>
<li>Takes each feature map output from convolutional layer and prepares a condensed feature map</li>
</ul>
</li>
<li>Final Layer Fully Connected:<ul>
<li>Prior to final layer we are creating some feature representation</li>
<li>Final layer is used to make a prediction</li>
</ul>
</li>
</ul>
<h3 id="deep-q-networks-dqn">Deep Q-Networks (DQN)</h3>
<ul><li>Represent value function, policy, and model using DNNs</li></ul>
<h3 id="dqns-in-atari">DQNs in Atari</h3>
<ul>
<li>End to end learning of values $Q(s, a)$ from pixels s<ul>
<li>Input state $s$ is stack of raw pixels from last 4 frames<ul><li>Allows you to grab velocity + position of the ball</li></ul>
</li>
<li>Output is $Q(s, a)$ for 18 joystick/button presses</li>
<li>Reward is change in score for that step</li>
<li>Same network architecture and hyperparameters across all games</li>
</ul>
</li>
<li>Minimize MSE loss with stochastic gradient</li>
<li>Divergence with Q-learning<ul>
<li>Correlations between samples</li>
<li>Non-stationary targets</li>
</ul>
</li>
<li>Address divergence with<ul>
<li>Experience Replay</li>
<li>Fixed Q Targets</li>
</ul>
</li>
</ul>
<h3 id="dqn-experienced-replay">DQN: Experienced Replay</h3>
<ul>
<li>To help remove correlations, store dataset (a reply buffer) from prior experience</li>
<li>To perform experience replay, repeat the following<ul>
<li>Sample an experience tuple from the dataset $(s, a, r, s’)$</li>
<li>Compute the target value for the sampled $s: r + \gamma max _{a’}\hat{Q}(s’, a’; w)$</li>
<li>Use stochastic gradient descent to update weights<ul><li>$\Delta w = \alpha (r + \gamma max _{a’}\hat{Q}(s’, a’; w) - \hat{Q} (s, a; w))\nabla_w \hat{Q}(s, a; w)$</li></ul>
</li>
</ul>
</li>
</ul>
<h3 id="fixed-q-targets">Fixed Q Targets</h3>
<ul><li>To improve stability, fix the target weights used in the target calculation for multiple updates<ul>
<li>Fix the w in $r + \gamma V(s’; w)$ for several rounds<ul><li>Approximation of the oracle of the $V^\ast$</li></ul>
</li>
<li>Use a different set weights to compute target than that is being updated<ul>
<li>$w^-$: Set of weights used for target computation<ul><li>Gets updated periodically (i.e., every $n$ steps $w^- = w$)</li></ul>
</li>
<li>$w$: Set of weights being updated</li>
</ul>
</li>
<li>Computation of target changes: $r + \gamma max _{a’}\hat{Q}(s’, a’; w^-)$<ul><li>SGD: $\Delta w = \alpha (r + \gamma max _{a’}\hat{Q}(s’, a’; w^-) - \hat{Q} (s, a; w))\nabla_w \hat{Q}(s, a; w)$</li></ul>
</li>
</ul>
</li></ul>
<h3 id="double-dqn">Double DQN</h3>
<ul><li>Similar idea to Double Q Learning but we have one network to selection actions and one network to evaluate actions<ul>
<li>$w$: Weights for network used to select actions</li>
<li>$w^-$: Weights for network used to evaluate actions</li>
<li>$\Delta w = \alpha (r + \gamma \hat{Q}(argmax _{a’} \hat{Q}(s’, a’; w); w^-) - \hat{Q}(s, a; w))$<ul>
<li>Action Selection: $argmax _{a’} \hat{Q}(s’, a’; w)$</li>
<li>Action Evaluation: $\hat{Q}(argmax _{a’} \hat{Q}(s’, a’; w); w^-)$</li>
</ul>
</li>
<li>Swap the $w^-$ and $w$ on each timestep which ensures both sets of weights get updated frequently</li>
<li>Avoids maximization bias</li>
</ul>
</li></ul>
<h3 id="prioritized-replay">Prioritized Replay</h3>
<ul>
<li>Prioritizing which replay you sample leads to exponential improvements in covergence (if we had a perfect oracle that could tell us the next replay to choose)</li>
<li>Heuristic: Prioritize Tuples Based on DQN Error:<ul>
<li>Let i be the index of the tuple of experience $(s_i, a_i, r_i, s _{i+1})$</li>
<li>Sample tuples using priority function</li>
<li>Priority of tuple is proportional to DQN error: $p_i = \vert r + \gamma max _{a’} Q(s _{i+1}, a’; w^-) - Q(s_i, a_i; w)\vert$</li>
<li>Update $p_i$ every update (initially set to 0)</li>
<li>Probability of selecting that tuple: $P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$</li>
</ul>
</li>
</ul>
<h3 id="dueling-dqn">Dueling DQN</h3>
<ul>
<li>Intuition: Features needed for value are not necessarily the features you need to determine the benefit of an action</li>
<li>
<strong>Advantage Function:</strong> $A^\pi(s, a) = Q^\pi(s,a) - V^\pi(s)$</li>
<li>Dueling DQNs seperate the value function and advantage function and estimate them seperately and then recombine them for the Q function</li>
<li>Not identifiable $\rightarrow$ given a $Q^\pi$ we cannot decompose it into a unique $A^\pi$ and $V^\pi$</li>
</ul></section></main></body>
</html>
