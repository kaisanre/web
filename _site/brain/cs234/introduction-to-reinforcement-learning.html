<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="lecture 1 - introduction to reinforcement learning">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="Resources: Lecture Video">
<meta property="og:description" content="Resources: Lecture Video">
<link rel="canonical" href="http://localhost:4000/brain/cs234/introduction-to-reinforcement-learning">
<meta property="og:url" content="http://localhost:4000/brain/cs234/introduction-to-reinforcement-learning">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-05-02T00:00:00-04:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="lecture 1 - introduction to reinforcement learning">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2024-05-02T00:00:00-04:00","datePublished":"2024-05-02T00:00:00-04:00","description":"Resources: Lecture Video","headline":"lecture 1 - introduction to reinforcement learning","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/brain/cs234/introduction-to-reinforcement-learning"},"url":"http://localhost:4000/brain/cs234/introduction-to-reinforcement-learning"}</script><title> lecture 1 - introduction to reinforcement learning - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.png">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="http://localhost:4000/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">writing</a></li>
<li><a href="/about">about</a></li>
<li><a href="/search">search</a></li>
<li><a href="/brain">brain</a></li>
</ul></nav></header><section class="post"><h2>
<a href="/brain/cs234" class="url">cs234</a> / lecture 1 - introduction to reinforcement learning</h2>
<p><strong>Resources:</strong></p>
<ul><li><a href="https://youtu.be/FgzM3zpZ55o?feature=shared">Lecture Video</a></li></ul>
<h3 id="introduction">Introduction</h3>
<p><strong>Goal:</strong> Use data / experience to make the best sequence of good decisions under certainty <strong>Credit Assignment Problem:</strong> The causal relationship between actions and future rewards</p>
<table>
<thead><tr>
<th style="text-align: left"> </th>
<th style="text-align: left">Optimization</th>
<th style="text-align: left">Exploration</th>
<th style="text-align: left">Generalization</th>
<th style="text-align: left">Delayed Consequences</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align: left"><strong>Reinforcement Learning</strong></td>
<td style="text-align: left">✅</td>
<td style="text-align: left">✅</td>
<td style="text-align: left">✅</td>
<td style="text-align: left">✅</td>
</tr>
<tr>
<td style="text-align: left"><strong>Planning</strong></td>
<td style="text-align: left" colspan="2">✅</td>
<td style="text-align: left">✅</td>
<td style="text-align: left">✅</td>

</tr>
<tr>
<td style="text-align: left"><strong>Supervised Machine Learning</strong></td>
<td style="text-align: left" colspan="2">✅</td>
<td style="text-align: left" colspan="2">✅</td>


</tr>
<tr>
<td style="text-align: left"><strong>Unsupervised Machine Learning</strong></td>
<td style="text-align: left" colspan="3">✅</td>
<td style="text-align: left">✅</td>


</tr>
<tr>
<td style="text-align: left"><strong>Imitation Learning</strong></td>
<td style="text-align: left" colspan="2">✅</td>
<td style="text-align: left">✅</td>
<td style="text-align: left">✅</td>

</tr>
</tbody>
</table>
<h3 id="imitation-learning">Imitation Learning</h3>
<p>Learning to do something by observing another agent do that task.</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Great tools for supervised learning</li>
<li>Avoids exploration problem</li>
<li>When there is lots of data, we have data over many outcomes</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Expensive to capture</li>
<li>Limited by data collected</li>
</ul>
<h3 id="sequential-decision-making">Sequential Decision Making</h3>
<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoiZ3JhcGggVEQ7XG5hZ2VudC0tPnxhY3Rpb258d29ybGQ7XG53b3JsZC0tPnxyZXdhcmR8YWdlbnQ7IiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifX0"></p>
<p><strong>Goal:</strong> Maximize total expected future reward</p>
<ul>
<li>balance long-term and immediate rewards</li>
<li>require strategic behavior to achieve high rewards</li>
</ul>
<p><strong>History:</strong> $h_t = (a_1, o_1, r_1…a_t, o_t, r_t)$</p>
<ul>
<li>$a$: the action</li>
<li>$o$: the observation</li>
<li>$r$: the reward</li>
<li>$t$: the time (discrete time period)</li>
<li>state, $s_t = (h_t)$, is a function of history</li>
</ul>
<p><strong>World State:</strong> The true state of the world generates next state + reward. This is usually unknown to the agent</p>
<p><strong>Markov Assumption:</strong> To predict the future, you only need to know the current state (future independent of past given the present) $$p(s _{t+1}| s_t, a_t) = p(s _{t+1}| h_t, a_t) $$</p>
<p>Setting the state as the history will always make the problem markov (but that is a lot information $\rightarrow$ using most recent observation for state is generally enough)</p>
<h3 id="observability">Observability</h3>
<ul>
<li>
<strong>Fully Observable World:</strong> Agent state and world state are the same $\rightarrow s_t = o_t$</li>
<li>
<strong>Partially Observable World:</strong> Agent state and the world state are not the same $\rightarrow$ agent constructs its own state. Uses history, beliefs about the world, etc. to construct its own state. Examples: Poker (you only see your own cards), Healthcare (don’t see all physiological processes)</li>
</ul>
<h3 id="types-of-sequential-decision-processes">Types of Sequential Decision Processes</h3>
<ul>
<li>
<strong>Bandits:</strong> Actions have no influence on next observations and no delayed rewards</li>
<li>
<strong>MDPs and POMDPs:</strong> Actions influence observations</li>
<li>
<strong>Deterministic:</strong> Given a history and action, there is a single observation and reward</li>
<li>
<strong>Stochastic:</strong> Given a history and action, there are many potential observations and rewards</li>
</ul>
<h3 id="rl-algorithm-components">RL Algorithm Components</h3>
<ul>
<li>
<strong>Model:</strong> Representation of how the world changes in response to an agent’s action<ul>
<li>Transition: $p(s _{t+1} = s’ \vert s _t, a _t)$</li>
<li>Reward: $r(s_t = s, a_t = a) = E[r_t \vert s_t = s, a_t = a]$</li>
</ul>
</li>
<li>
<strong>Policy:</strong> Function mapping of agent’s states to action<ul>
<li>Deterministic: One action per state</li>
<li>Stochastic: Distribution of actions per state</li>
</ul>
</li>
<li>
<strong>Value Function:</strong> Future rewards from being in a state and/or action when following a policy<ul>
<li>Expected discounted sum of rewards</li>
<li>Formula: $V^\pi(s_t = s) = E _{\pi}[r_t + \gamma r _{t+1}+ \gamma^2 r _{t+2} + \dots \vert s_t = s]$</li>
<li>Discount Factor: weighs immediate vs future rewards</li>
<li>Quantify the goodness or badness of states and actions</li>
</ul>
</li>
</ul>
<h3 id="types-of-rl-agents">Types of RL Agents</h3>
<ul>
<li>
<strong>Model Based:</strong> Explicit model $\rightarrow$ may or may not have a policy and/or value function</li>
<li>
<strong>Model Free:</strong> No explicit model $\rightarrow$ explicit policy and/or value function</li>
</ul>
<h3 id="challenges-in-rl">Challenges in RL</h3>
<ul>
<li>Agent doesn’t know how the world works</li>
<li>Agent needs to know how to interact with the world to make good decisions</li>
<li>Agent needs to figure out how to improve policy</li>
</ul>
<h3 id="exploration-and-exploitation">Exploration and Exploitation</h3>
<ul>
<li>Exploration: Trying new things to enable the agent to make better decisions in the future</li>
<li>Exploitation: Choosing actions that are expected to yield good rewards given past experience</li>
</ul>
<h3 id="evaluation-and-control">Evaluation and Control</h3>
<ul>
<li>Evaluation: Given a policy, estimate the reward</li>
<li>Control: Find the best policy</li>
</ul></section></main></body>
</html>
