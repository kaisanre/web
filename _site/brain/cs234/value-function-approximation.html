<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="lecture 5 - value function approximation">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="Resources: Lecture Video">
<meta property="og:description" content="Resources: Lecture Video">
<link rel="canonical" href="http://localhost:4000/brain/cs234/value-function-approximation">
<meta property="og:url" content="http://localhost:4000/brain/cs234/value-function-approximation">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-05-07T00:00:00-04:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="lecture 5 - value function approximation">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2024-05-07T00:00:00-04:00","datePublished":"2024-05-07T00:00:00-04:00","description":"Resources: Lecture Video","headline":"lecture 5 - value function approximation","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/brain/cs234/value-function-approximation"},"url":"http://localhost:4000/brain/cs234/value-function-approximation"}</script><title> lecture 5 - value function approximation - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.png">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="http://localhost:4000/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">writing</a></li>
<li><a href="/about">about</a></li>
<li><a href="/search">search</a></li>
<li><a href="/brain">brain</a></li>
</ul></nav></header><section class="post"><h2>
<a href="/brain/cs234" class="url">cs234</a> / lecture 5 - value function approximation</h2>
<p><strong>Resources:</strong></p>
<ul><li><a href="https://youtu.be/buptHUzDKcE?feature=shared">Lecture Video</a></li></ul>
<h3 id="value-function-approximation">Value Function Approximation</h3>
<ul>
<li>Represet the state / state-action value function as a parameterized function instead of table<ul>
<li>$\hat{V}(s; w)$</li>
<li>$\hat{Q}(s, a; w)$<ul><li>$w$ is a vector of parameters of a deep neural network or something simpler</li></ul>
</li>
</ul>
</li>
<li>This allows for generalization<ul>
<li>Reduces the memory, computation, and experience needed to find a good $P, R / V / Q / \pi$</li>
<li>Trade offs between representational capacity and memory, computation, data</li>
</ul>
</li>
<li>Possible Function Approximators<ul>
<li>Linear Combinations</li>
<li>Neural Networks</li>
<li>Decision Trees</li>
<li>Nearest neighbors</li>
<li>Fourier/wavelet bases</li>
</ul>
</li>
</ul>
<h3 id="review-gradient-descent">Review: Gradient Descent</h3>
<ul><li>Given a function $J(w)$ that is differentiable to $w$<ul>
<li>Find a $w$ that minimizes J</li>
<li>Gradient: $\nabla w J(w) = \frac{dJ(w)}{dw_1}\frac{dJ(w)}{dw_2}\dots$</li>
<li>Move parameter vector in the direction of the gradient<ul><li>$\overrightarrow{w} = \overrightarrow{w} - \alpha(\nabla w J(w))$<ul><li>$\alpha$ is the learning rate</li></ul>
</li></ul>
</li>
</ul>
</li></ul>
<h3 id="value-function-approximation-for-policy-evaluation">Value Function Approximation for Policy Evaluation</h3>
<ul>
<li>Core Idea: We want to find the best representation in our space for state value pairs $(s_1, V^\pi(s_1))$</li>
<li>In the context of stochastic gradient descent:<ul>
<li>Minimize our loss between true value function $V^\pi(s)$ and its approximation $\hat{V}(s; w)$</li>
<li>Use mean square error: $J(w) = E_{\pi}[(V^\pi(s) - \hat{V}(s; w))^2]$<ul><li>Perform gradient descent on this: $\Delta w = \frac{-1}{2}\alpha\nabla_w J(w)$</li></ul>
</li>
<li>Use stochastic gradient descent to calculate $\Delta w$ for a single point</li>
</ul>
</li>
<li>Issue: we do not have an oracle that will give us the true value function of a state ($V^\pi(s)$)</li>
</ul>
<h3 id="model-free-value-function-approximation">Model Free Value Function Approximation</h3>
<ul>
<li>In model free policy evaluation we:<ul>
<li>Followed a fixed policy</li>
<li>Needed to estimate the state value and state-action value functions</li>
</ul>
</li>
<li>During the estimate step, we also will now fit the function approximator</li>
</ul>
<h3 id="feature-factors">Feature Factors</h3>
<ul><li>If a state vector is partially aliased (no information to estimate an effect once earlier features have been fit), then it is not markov</li></ul>
<h3 id="linear-value-function-approximation">Linear Value Function Approximation</h3>
<ul>
<li>Features encode states for policy evaluation</li>
<li>We can represent a value function (or state-action value function) for a policy as a weighted linear combination of features<ul>
<li>$\hat{V}(s; w) = \sum _{j= 1}^n x_j(s)w_j = x(s)^Tw$</li>
<li>Objective is to minimize: $J(w) = E_{\pi}[(V^\pi(s) - \hat{V}(s; w))^2]$</li>
<li>Update = step size * prediction error * feature value<ul><li>$\Delta w = -\frac{1}{2}\alpha (2(V^\pi(s) - \hat{V^\pi}(s)))x(s)$</li></ul>
</li>
</ul>
</li>
</ul>
<h3 id="monte-carlo-value-function-approximation">Monte Carlo Value Function Approximation</h3>
<ul>
<li>Return $G_t$ as a noisy sample (estimate) of the true expected return</li>
<li>We can supervised learning on (state, return) pairs: $(s_1, G_1), (s_2, G_2) \dots$<ul><li>$\Delta w = \alpha (G_t - x(s_t)^Tw)x(s_t)$</li></ul>
</li>
<li>
<strong>Algorithm:</strong><ul>
<li>Initialize $w = 0, k = 1$</li>
<li>Loop<ul>
<li>sample kth episode $(s _{k,1}, a _{k,1}, r _{k,1}, s _{k,2}, a _{k,2}, r _{k,2}, \dots)$ given $\pi$</li>
<li>for $t = 1 \dots L_k$<ul><li>if First visit or every visit to (s) in episode k then<ul>
<li>$G_t(s) = \sum _{j=t}^{L_k} r _{k,j}$</li>
<li>Update weights<ul><li>$w = w - \alpha(G_t(s) - \hat{V}(s, w))x(s)$<ul><li>$\hat{V}(s, w) = x_s(w)$</li></ul>
</li></ul>
</li>
</ul>
</li></ul>
</li>
<li>$k = k+1$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="convergence-guarantees-for-linear-value-function-approximation-for-policy-evaluation">Convergence Guarantees for Linear Value Function Approximation for Policy Evaluation</h3>
<ul>
<li>A markov chain with an MDP with a particular policy will converge to a probability distribution over states, $d(s)$</li>
<li>$d(s)$: Stationary distribution over states of $\pi$<ul>
<li>$\sum_s d(s) = 1$</li>
<li>$d(s’) = \sum _{s} \sum _{a} \pi(s\vert a)p(s’\vert s, a)d(s)$</li>
</ul>
</li>
<li>We want to define the mean squared error of a linear value function approximation for a policy relative to the true value<ul>
<li>$MSVE(w) = \sum _{s \in S}d(s)(V^\pi(s) - \hat{V^\pi}(s; w))^2$<ul><li>Intuition: if there is a state that is visited rarely, then a bigger error is okay and vice versa</li></ul>
</li>
<li>$MSVE(w _{MC}) = min_w \sum _{s \in S}d(s)(V^\pi(s) - \hat{V^\pi}(s; w))^2$<ul><li>If you run monte carlo policy evaluation with VFA, this will converge to the best possible weights</li></ul>
</li>
</ul>
</li>
</ul>
<h3 id="batch-monte-carlo-value-function-approximation">Batch Monte Carlo Value Function Approximation</h3>
<ul>
<li>You have a set of episodes from a policy</li>
<li>Analytically solve for best linear approximation that minimizes mean squared error on that dataset</li>
<li>Let $G(s_i)$ be an unbiased sample of the true return, $V^\pi (s_i)$<ul>
<li>We can find the weights that minimize the error: $argmin_w\sum _{i=1}^N(G(s_i)-x(s_i)^Tw)^2$</li>
<li>Take the derivative and set it equal to 0 and then solve for w: $w = (X^TX)^{-1}X^TG$</li>
</ul>
</li>
</ul>
<h3 id="temporal-difference-learning-with-value-function-approximation">Temporal Difference Learning with Value Function Approximation</h3>
<ul>
<li>Our target value is $r + \gamma \hat{V}^\pi(s’;w)$ - however this is biased estimate of the true value of $V^\pi$</li>
<li>Instead we can do TD learning with value function approximation on a set of data pairs $(s_1, r_1 + \hat{V}\pi(s_2; w)), (s_2, r_2 + \hat{V}\pi(s_3; w)) \dots$</li>
<li>Then we find weights to minimize: $J(w) = E _\pi[(r_j + \hat{V}\pi(s _{j+1}; w) - \hat{V}(s_j; w))^2]$</li>
<li>TD(0) Difference: $\Delta w = \alpha(r + \gamma x(s’)^Tw-x(s)^Tw)x(s)$</li>
<li>
<strong>Algorithm:</strong><ul>
<li>Initialize $w = 0, k = 1$</li>
<li>Loop:<ul>
<li>Sample a tuple $(s_k, a_k, r_k, s _{k+1})$</li>
<li>Update Weights: $w = w + \alpha(r + \gamma x(s’)^Tw-x(s)^Tw)x(s)$</li>
<li>$k = k+1$</li>
</ul>
</li>
</ul>
</li>
<li>Convergence:<ul>
<li>$MSVE(w _{TD}) = \frac{1}{1-\gamma}min_w \sum _{s \in S}d(s)(V^\pi(s) - \hat{V^\pi}(s; w))^2$</li>
<li>Weights generateed from convergence is within a constant factor of the minimum MSVE error possible<ul><li>Not quite as good as Monte Carlo</li></ul>
</li>
</ul>
</li>
</ul>
<h3 id="control-using-value-function-approximation">Control Using Value Function Approximation</h3>
<ul>
<li>$\hat{Q}^\pi(s, a; w) \approx Q^\pi$</li>
<li>Approximate policy evaluation using value function approximation</li>
<li>Perform $\epsilon$-greedy policy improvement</li>
<li>Can be unstable because of the intersection of function approximation, sampling, bootstrapping, and off-policy learning</li>
</ul>
<h3 id="action-value-approximation-with-an-oracle">Action-Value Approximation with an Oracle</h3>
<ul><li>Minimize the following with stochastic gradient descent: $J(w) = E_{\pi}[(Q^\pi(s, a) - \hat{V}(s, a; w))^2]$</li></ul>
<h3 id="linear-state-action-value-approximation-with-an-oracle">Linear State Action Value Approximation with an Oracle</h3>
<ul><li>Features encode states and actions<ul>
<li>$\hat{Q}(s, a; w) = \sum _{j= 1}^n x_j(s, a)w_j = x(s, a)^Tw$</li>
<li>Use stochastic gradient descent to update: $\nabla_w J(w) = \nabla_w E_{\pi}[(Q^\pi(s, a) - \hat{V}(s, a; w))^2]$</li>
</ul>
</li></ul>
<h3 id="incremental-model-free-control-approaches">Incremental Model-Free Control Approaches</h3>
<ul><li>Similar to policy evaluation: true state-action value for a function is unknown so just substitute a target value<ul>
<li>Monte Carlo: $\Delta w = \alpha (G_t - \hat{Q}(s_t, a_t;w))\nabla_w\hat{Q}(s_t, a_t;w)$</li>
<li>SARSA: $\Delta w = \alpha (r + \gamma \hat{Q}^\pi(s’, a’;w) - \hat{Q}(s_t, a_t;w))\nabla_w\hat{Q}(s_t, a_t;w)$</li>
<li>Q-Learning: $\Delta w = \alpha (r + \gamma max _{a’} \hat{Q}^\pi(s’, a’;w) - \hat{Q}(s_t, a_t;w))\nabla_w\hat{Q}(s_t, a_t;w)$</li>
</ul>
</li></ul>
<h3 id="convergence-of-td-methods-with-vfa">Convergence of TD Methods with VFA</h3>
<ul><li>Value function approximation is not necessarily a contraction (like with bellman operators)<ul><li>This means the value can diverge</li></ul>
</li></ul>
<p><strong>Convergence Guarantees:</strong></p>
<table>
<thead><tr>
<th style="text-align: left"> </th>
<th style="text-align: left">Tabular</th>
<th style="text-align: left">Linear VFA</th>
<th style="text-align: left">Nonlinear VFA</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align: left"><strong>Monte Carlo Control</strong></td>
<td style="text-align: left">✅</td>
<td style="text-align: left">✅(might be oscillation)</td>
<td style="text-align: left">❌</td>
</tr>
<tr>
<td style="text-align: left"><strong>SARSA</strong></td>
<td style="text-align: left">✅</td>
<td style="text-align: left">✅ (might be oscillation)</td>
<td style="text-align: left">❌</td>
</tr>
<tr>
<td style="text-align: left"><strong>Q-Learning</strong></td>
<td style="text-align: left">✅</td>
<td style="text-align: left">❌</td>
<td style="text-align: left">❌</td>
</tr>
</tbody>
</table></section></main></body>
</html>
