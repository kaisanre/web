<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet">
<meta name="generator" content="Jekyll v4.2.2">
<meta property="og:title" content="background lecture 0 - markov decision processes value iteration">
<meta name="author" content="samrat sahoo">
<meta property="og:locale" content="en_US">
<meta name="description" content="Note: This was from stanford cs221 and is not part of cs234. I used this to gain some prerequistie knowledge for cs234">
<meta property="og:description" content="Note: This was from stanford cs221 and is not part of cs234. I used this to gain some prerequistie knowledge for cs234">
<link rel="canonical" href="http://localhost:4000/brain/cs234/markov-decision-processes-value-iteration">
<meta property="og:url" content="http://localhost:4000/brain/cs234/markov-decision-processes-value-iteration">
<meta property="og:site_name" content="samrat’s thought space">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-05-01T00:00:00-04:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="background lecture 0 - markov decision processes value iteration">
<meta name="twitter:site" content="@samratdotjs">
<meta name="twitter:creator" content="@samratdotjs"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"samrat sahoo","url":"https://samratsahoo.com"},"dateModified":"2024-05-01T00:00:00-04:00","datePublished":"2024-05-01T00:00:00-04:00","description":"Note: This was from stanford cs221 and is not part of cs234. I used this to gain some prerequistie knowledge for cs234","headline":"background lecture 0 - markov decision processes value iteration","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/brain/cs234/markov-decision-processes-value-iteration"},"url":"http://localhost:4000/brain/cs234/markov-decision-processes-value-iteration"}</script><title> background lecture 0 - markov decision processes value iteration - samrat's thought space</title>
<link rel="shortcut icon" href="/favicon.png">
<link rel="alternate" type="application/atom+xml" title="samrat's thought space" href="/atom.xml">
<link rel="alternate" type="application/json" title="samrat's thought space" href="http://localhost:4000/feed.json">
<link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml">
<style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}img.mermaid{max-width:200px}.url{color:#0645AD}*::-moz-selection{color:white;background:#7011dc}*::selection{color:white;background:#7011dc}*{font-family:"Outfit", sans-serif}li>ul{padding-left:1rem}</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">samrat's thought space</h1>--><nav role="navigation" aria-hidden="true"><ul>
<li><a href="/">writing</a></li>
<li><a href="/about">about</a></li>
<li><a href="/search">search</a></li>
<li><a href="/brain">brain</a></li>
</ul></nav></header><section class="post"><h2>
<a href="/brain/cs234" class="url">cs234</a> / background lecture 0 - markov decision processes value iteration</h2>
<p><em>Note: This was from stanford cs221 and is not part of cs234. I used this to gain some prerequistie knowledge for cs234</em></p>
<p><strong>Resources:</strong></p>
<ul><li><a href="https://youtu.be/9g32v7bK3Co?feature=shared">Lecture Video</a></li></ul>
<h3 id="introduction">Introduction</h3>
<p>Based on uncertainty / randomness of the real world where a state + action combo can lead to two different states.</p>
<p><strong>Applications:</strong></p>
<ul>
<li>Robotics (actuators can fail, unseen obstacles)</li>
<li>Resource Allocation (do not know customer demand for various products)</li>
<li>Agriculture (uncertain weather impacts crop yield)</li>
</ul>
<h3 id="markov-decision-processes">Markov Decision Processes</h3>
<p><strong>Transition State Diagram:</strong></p>
<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoiZ3JhcGggVEQ7XG5Jbi0tPkluLFN0YXk7XG5Jbi0tPkluLFF1aXQ7XG5JbixRdWl0LS0-fDF8RW5kO1xuSW4sU3RheS0tPnwyLzN8IEVuZDtcbkluLFN0YXktLT58MS8zfEluOyIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In19"></p>
<p>Blocks represent the state nodes/ state-action node. Label edges represent the probability of that transition occuring from that state-action node.</p>
<p><strong>Transition Function:</strong> Gives you the probability of going from a given state to another state given an action.</p>
<ul>
<li>$T(s, a, s’) &gt; 0$ where $s$ is a given state, $a$ is an action, and $s’$ is the next state</li>
<li>Adding up all possible $s’$ you can end up at for the same $s, a$ should result in 1<ul><li>$\sum_{s’ \in \text{states}}T(s, a, s’) = 1$</li></ul>
</li>
</ul>
<p><strong>Reward Function:</strong> Gives you the reward of the transition from one state to another given an action</p>
<ul><li>$R(s, a, s’)$ where $s$ is a given state, $a$ is an action, and $s’$ is the next state</li></ul>
<p><strong>Markov:</strong> Future states only depend on the current state and action taken.</p>
<p><strong>Policy ($\pi$):</strong> A mapping from each state $s \in \text{states}$ to each action $a \in \text{Actions}(s)$</p>
<ul><li>This is the solution to an MDP</li></ul>
<p><strong>Discount Factor:</strong> The value of future rewards relative to your current day rewards</p>
<ul>
<li>Range: $0 \leq \gamma \leq 1$</li>
<li>1 means save for the future</li>
<li>0 means live in the moment</li>
</ul>
<h3 id="policy-evaluation">Policy Evaluation</h3>
<p><strong>Utility:</strong> The discounted sum of rewards over a random path that is yielded by a specific policy.</p>
<ul>
<li>Utility is a random variable</li>
<li>Formula: $U = r_1 + \gamma r_2 + \gamma^2 r_3 + ….$</li>
</ul>
<p><strong>Value:</strong> The expected utility of a specific policy</p>
<ul>
<li>Average of utilities on all random paths</li>
<li>Not a random variable</li>
</ul>
<p><strong>Value of a Policy</strong>: Expected utility received by following policy $\pi$ from state $s$ - denoted by $V_\pi(s)$</p>
<p><strong>Q-Value of a Policy</strong>: Expected utility from a state-action node - denoted by $Q_\pi(s, a)$</p>
<p>Recurrence Using $V_\pi(s)$ and $Q_\pi(s, a)$:</p>
<p>$$ V_\pi(s) = \begin{cases} { 0 \text{ if } \text{IsEnd}(s) = \text{True, } \text{else } Q_\pi(s, a)} \end{cases} $$ $$Q_\pi(s, a) = \sum_{s’}T(s, a, s’)[R(s, a, s’) + \gamma V_\pi(s’)]$$</p>
<p><strong>Iterative Algorithm:</strong> Start with arbitrary policy values and apply the recurrence until convergence</p>
<ul>
<li>Step 1. Initialize $V^0_\pi(s) = 0$ for all states $s$.</li>
<li>Step 2. Iterate until convergence (keep track of a time, t)</li>
<li>Step 3. For each state s update $V^t = \sum_{s’}T(s, \pi(s), s’)[R(s, \pi(s), s’) + \gamma V^{(t-1)}_\pi(s’)]$</li>
<li>Note: $Q^{(t-1)}(s, \pi(s)) = V^t_\pi(s)$</li>
</ul>
<p><strong>Implementation Details:</strong> We want to wait until convergence but that might take a while so we use this heuristic:</p>
<p>$$\text{max}_{s \in \text{states}} V^t _\pi(s) - V^{t-1} _\pi(s) \leq \epsilon$$</p>
<ul><li>Only need to store last two iterations of $V^t_\pi(s)$, not all</li></ul>
<h3 id="value-iteration">Value Iteration</h3>
<p><strong>Optimal Value:</strong> The maximum value obtained by any policy - denoted by $V_{opt}(s)$</p>
<p>$$Q_{opt}(s, a) = \sum_{s’}T(s, a, s’)[R(s, a, s’) + \gamma V_{opt}(s’)]$$ $$ V_{\text{opt}}(s) = \begin{cases} 0 \text{ if } \text{IsEnd}(s) = \text{True, }<br> \text{else } \max_{a \in \text{Actions}(s)} Q_\pi(s, a) \end{cases} $$ $$\pi_{opt}(s) = argmax_{a \in \text{Actions(s) }} \text{ }Q_{opt}(s,a)$$</p>
<p><strong>Iterative Algorithm:</strong></p>
<ul>
<li>Step 1. Initialize $V^0_{opt}(s) = 0$ for all states $s$.</li>
<li>Step 2. Iterate until convergence (keep track of a time, t)</li>
<li>Step 3. For each state s update $V_{opt}^t = \text{max}_{a \in \text{Actions(s)}} (\sum _{s’}T(s, a, s’)[R(s, a, s’) + \gamma V^{(t-1)} _{opt}(s’)])$</li>
<li>Note: $Q^{(t-1)}(s, a) = V^t_{opt}(s)$</li>
</ul></section></main></body>
</html>
